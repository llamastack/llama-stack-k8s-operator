# Example: Resource Registration (User Story 4)
#
# This example demonstrates registering models, tools, and shields:
# - Models can be simple strings or detailed configurations
# - Models without explicit provider use first inference provider
# - Tools map to tool_groups in config.yaml
# - Shields map to shields in config.yaml
#
# Use cases:
# - Registering LLM models with inference providers
# - Adding tool capabilities (code interpreter, web search)
# - Configuring content moderation with shields
#
apiVersion: llama.stacklok.com/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: with-resources
  namespace: llama-stack
spec:
  replicas: 1
  server:
    distribution:
      name: starter
    providers:
      inference:
        provider: vllm
        endpoint: http://vllm:8000
      safety:
        provider: llama-guard
        endpoint: http://llama-guard:8000
    # Register models, tools, and shields
    resources:
      # Simple model registration (uses first inference provider)
      models:
        - name: meta-llama/Llama-3.2-3B-Instruct
        - name: meta-llama/Llama-3.2-7B-Instruct
      # Tool groups to enable
      tools:
        - builtin::code_interpreter
        - builtin::websearch
      # Safety shields to enable
      shields:
        - meta-llama/Llama-Guard-3-8B
---
# Example: Detailed model registration with metadata
apiVersion: llama.stacklok.com/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: detailed-models
  namespace: llama-stack
spec:
  replicas: 1
  server:
    distribution:
      name: starter
    providers:
      inference:
        - id: vllm-gpu
          provider: vllm
          endpoint: http://vllm-gpu:8000
        - id: vllm-cpu
          provider: vllm
          endpoint: http://vllm-cpu:8000
    resources:
      models:
        # Large model on GPU with custom metadata
        - name: meta-llama/Llama-3.2-70B-Instruct
          provider: vllm-gpu
          metadata:
            contextLength: 8192
        # Smaller model on CPU
        - name: meta-llama/Llama-3.2-3B-Instruct
          provider: vllm-cpu
          metadata:
            contextLength: 4096
        # Embedding model with dimension
        - name: BAAI/bge-large-en-v1.5
          provider: vllm-cpu
          metadata:
            embeddingDimension: 1024
---
# Example: Full resource configuration
apiVersion: llama.stacklok.com/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: full-resources
  namespace: llama-stack
spec:
  replicas: 1
  server:
    distribution:
      name: starter
    providers:
      inference:
        provider: vllm
        endpoint: http://vllm:8000
      safety:
        provider: llama-guard
        endpoint: http://llama-guard:8000
      vectorIo:
        provider: pgvector
        host:
          secretKeyRef:
            name: postgres-credentials
            key: host
    resources:
      models:
        - name: meta-llama/Llama-3.2-3B-Instruct
        - name: meta-llama/Llama-3.2-7B-Instruct
      tools:
        - builtin::code_interpreter
        - builtin::websearch
        - builtin::memory
        - mcp::custom-tool-server
      shields:
        - meta-llama/Llama-Guard-3-8B
        - custom-content-filter

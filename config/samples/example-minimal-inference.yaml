# Example: Minimal single provider inference configuration
#
# This example demonstrates the simplest operator-generated config:
# - Single inference provider (vLLM)
# - No explicit ID needed for single provider
# - Operator generates config.yaml from this spec
#
# Prerequisites:
# - A vLLM server running at http://vllm:8000
# - (Optional) API key in a secret if vLLM requires authentication
#
apiVersion: llama.stacklok.com/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: minimal-inference
  namespace: llama-stack
spec:
  replicas: 1
  server:
    distribution:
      name: starter
    # Provider configuration - operator generates config.yaml from this
    providers:
      inference:
        provider: vllm
        endpoint: http://vllm:8000
    # Register a model to use with the inference provider
    resources:
      models:
        - name: meta-llama/Llama-3.2-3B-Instruct
---
# Example with API key authentication (e.g., for OpenAI)
apiVersion: llama.stacklok.com/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: openai-inference
  namespace: llama-stack
spec:
  replicas: 1
  server:
    distribution:
      name: starter
    providers:
      inference:
        provider: openai
        apiKey:
          secretKeyRef:
            name: openai-credentials
            key: api-key
    resources:
      models:
        - name: gpt-4o-mini
---
# Secret for OpenAI API key (create this separately with your actual key)
# kubectl create secret generic openai-credentials --from-literal=api-key=sk-...
apiVersion: v1
kind: Secret
metadata:
  name: openai-credentials
  namespace: llama-stack
type: Opaque
stringData:
  api-key: "your-openai-api-key-here"

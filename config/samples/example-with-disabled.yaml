# Example: Disable Providers/APIs (User Story 5)
#
# This example demonstrates disabling provider types:
# - Reduces attack surface by removing unused capabilities
# - Disabled provider types are completely removed from config.yaml
# - Warnings are logged if disabled type doesn't exist in distribution
#
# Valid disabled provider types:
# - inference, safety, vectorIo, agents, memory
# - toolRuntime, telemetry, datasetio, scoring, eval, postTraining
#
# Use cases:
# - Minimal inference-only deployment
# - Security-hardened configuration
# - Reducing resource usage
#
apiVersion: llama.stacklok.com/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: minimal-inference-only
  namespace: llama-stack
spec:
  replicas: 1
  server:
    distribution:
      name: starter
    providers:
      inference:
        provider: vllm
        endpoint: http://vllm:8000
    # Disable all non-essential APIs
    disabled:
      - safety
      - vectorIo
      - agents
      - memory
      - toolRuntime
      - telemetry
    resources:
      models:
        - name: meta-llama/Llama-3.2-3B-Instruct
---
# Example: Inference + Safety only (content moderation focus)
apiVersion: llama.stacklok.com/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: safe-inference
  namespace: llama-stack
spec:
  replicas: 1
  server:
    distribution:
      name: starter
    providers:
      inference:
        provider: vllm
        endpoint: http://vllm:8000
      safety:
        provider: llama-guard
        endpoint: http://llama-guard:8000
    # Keep only inference and safety
    disabled:
      - vectorIo
      - agents
      - memory
      - toolRuntime
      - telemetry
    resources:
      models:
        - name: meta-llama/Llama-3.2-3B-Instruct
      shields:
        - meta-llama/Llama-Guard-3-8B
---
# Example: RAG-focused deployment (inference + vector)
apiVersion: llama.stacklok.com/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: rag-focused
  namespace: llama-stack
spec:
  replicas: 1
  server:
    distribution:
      name: starter
    providers:
      inference:
        provider: vllm
        endpoint: http://vllm:8000
      vectorIo:
        provider: pgvector
        host:
          secretKeyRef:
            name: postgres-credentials
            key: host
    # Disable unused capabilities for RAG use case
    disabled:
      - safety
      - agents
      - toolRuntime
      - telemetry
    resources:
      models:
        - name: meta-llama/Llama-3.2-3B-Instruct
        - name: BAAI/bge-large-en-v1.5
          metadata:
            embeddingDimension: 1024

apiVersion: llama.x-k8s.io/v1alpha1
kind: LlamaStackDistribution
metadata:
  name: llamastackdistribution-sample
spec:
  replicas: 1
  server:
    distribution:
      name: ollama
    containerSpec:
      name: llama-stack
      port: 8321
      env:
      - name: INFERENCE_MODEL
        value: "meta-llama/Llama-3.2-3B-Instruct"
    # Uncomment the storage section to use persistent storage
    # storage: {}  # Will use default size of 10Gi and default mount path of /.llama
    # Or specify custom values:
    storage:
      size: "20Gi"
      mountPath: "/home/lls/.lls"  # Optional, defaults to /.llama. Use with custom distribution images that have a different setup.
